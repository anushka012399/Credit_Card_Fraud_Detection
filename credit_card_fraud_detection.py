# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GGaClGSvruTZ_XgKvY-8Hujz2z7nhPgI
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# %matplotlib inline

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression 
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier


from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import recall_score, precision_score, f1_score

import statsmodels.api as sm

import sys
np.set_printoptions(threshold=sys.maxsize)

!pip install --quiet kaggle

from google.colab import files 
files.upload() #upload the json file that contains api key from kaggle account

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/.  

!chmod 600 ~/.kaggle/kaggle.json #altering permissions

!kaggle datasets download -d mlg-ulb/creditcardfraud

from zipfile import ZipFile
zip_file= ZipFile('creditcardfraud.zip') 
data=pd.read_csv(zip_file.open('creditcard.csv'))

data.head()

data.info()

sns.countplot(x='Class',data=data, color='b')

fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))
ax1.hist(data.Time[data.Class==0],bins=48,color='b',alpha=0.5)
ax1.set_title('Genuine')
ax2.hist(data.Time[data.Class==1],bins=48,color='black',alpha=0.5)
ax2.set_title('Fraud')
plt.xlabel('Time (seconds)')
plt.ylabel('No. of transactions')

fig, (ax3,ax4) = plt.subplots(2,1, figsize = (6,3), sharex = True)
ax3.hist(data.Amount[data.Class==0],bins=50,color='b',alpha=0.5)
ax3.set_yscale('log')
ax3.set_title('Legitimate') 
ax3.set_ylabel('transactions')
ax4.hist(data.Amount[data.Class==1],bins=50,color='black',alpha=0.5)
ax4.set_yscale('log') 
ax4.set_title('Fraud') 
ax4.set_xlabel('Amount ($)')
ax4.set_ylabel('transactions')

data.columns

b=data.shape[0]
data.drop_duplicates(subset=data.columns.values[:-1], keep='first',inplace=True)
print(b-data.shape[0]," duplicated Rows has been removed")

data.Class.value_counts()

plt.subplots(figsize=(20,20))
sns.heatmap(data.corr(), vmax=.8 , square=True,annot=True,fmt='.2f',cmap="Blues")

data.corr().nlargest(31,'Class')['Class']

#But before let's not forget to transform the train data and test data to make the algorithm work faster
df = pd.DataFrame(StandardScaler().fit_transform(data), columns=data.columns, index=data.index)

"""# **Naive Bayes**"""

def split_data(df, drop_list):
    df = df.drop(drop_list,axis=1)
    print(df.columns)
    y = df['Class'].values 
    X = df.drop(['Class'],axis=1).values 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)

    print("train-set size: ", len(y_train),
      "\ntest-set size: ", len(y_test))
    print("fraud cases in test-set: ", sum(y_test))
    return X_train, X_test, y_train, y_test

drop_list = []
X_train, X_test, y_train, y_test = split_data(data, drop_list)
naivebayes = GaussianNB()
naivebayes.fit(X_train,y_train,None)
y_pred = naivebayes.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

drop_list = ['V22','V26','V25','V15','V13','V23','V24']
X_train, X_test, y_train, y_test = split_data(data, drop_list)
naivebayes = GaussianNB()
naivebayes.fit(X_train,y_train,None)
y_pred = naivebayes.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

drop_list = ['Time','V22','V26','V25','V15','V13','V23','V24','Amount']
X_train, X_test, y_train, y_test = split_data(data, drop_list)
naivebayes = GaussianNB()
naivebayes.fit(X_train,y_train,None)
y_pred = naivebayes.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

"""# **Logistic Regression**"""

y = data['Class'].values 
X = data.drop(['Class'],axis=1).values 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)
print("train-set size: ", len(y_train),"\ntest-set size: ", len(y_test))
print("fraud cases in test-set: ", sum(y_test))
lrmodel=LogisticRegression(C = 0.01)
lrmodel.fit(X_train,y_train,None)
y_pred = lrmodel.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

fraud_ind = np.array(data[data.Class == 1].index)
gen_ind = data[data.Class == 0].index
n_fraud = len(data[data.Class == 1])

random_gen_ind = np.random.choice(gen_ind, n_fraud, replace = False)
random_gen_ind = np.array(random_gen_ind)

under_sample_ind = np.concatenate([fraud_ind,random_gen_ind])

# Under sample dataset
undersample_df = data.loc[under_sample_ind,:]
y_undersample  = undersample_df['Class'].values #target
X_undersample = undersample_df.drop(['Class'],axis=1).values #features

print("# transactions in undersampled data: ", len(undersample_df))
print("% genuine transactions: ",len(undersample_df[undersample_df.Class == 0])/len(undersample_df))
print("% fraud transactions: ", sum(y_undersample)/len(undersample_df))

y = undersample_df['Class'].values 
X = undersample_df.drop(['Class'],axis=1).values 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)
print("train-set size: ", len(y_train),"\ntest-set size: ", len(y_test))
print("fraud cases in test-set: ", sum(y_test))
lrmodel=LogisticRegression(C = 0.01)
lrmodel.fit(X_train,y_train,None)
y_pred = lrmodel.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

"""# **K-NEAREST NEIGHBOUR**"""

y = data['Class'].values 
X = data.drop(['Class'],axis=1).values 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=40, stratify=y)
print("train-set size: ", len(y_train),"\ntest-set size: ", len(y_test))
print("fraud cases in test-set: ", sum(y_test))
knnmodel = KNeighborsClassifier(n_neighbors=100)
knnmodel.fit(X_train, y_train)

y_pred = knnmodel.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))

knnmodel = KNeighborsClassifier(n_neighbors=5)
knnmodel.fit(X_train, y_train)

y_pred = knnmodel.predict(X_test)
sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')
print("recall score: ", recall_score(y_test,y_pred))
print("precision score: ", precision_score(y_test,y_pred))
print("f1 score: ", f1_score(y_test,y_pred))
print("accuracy score: ", accuracy_score(y_test,y_pred))







